\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size, margins, and layout properties
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\setlength{\parskip}{\baselineskip}

%% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{physics}
\usepackage{array}

%% Math Operations
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{LaTeX Equation Practice}
\author{Raymond Heberer\\
  \texttt{ray.heberer@gmail.com}}
  
\usepackage{datetime}
\newdate{date}{25}{05}{2018}
\date{\displaydate{date}}

\begin{document}
\maketitle

\section{Introduction}

\paragraph{} Ideas from mathematics underly virtually every technique and concept in Machine Learning (ML). While possessing a rigorous understanding of all these ideas is certainly not required, it can be beneficial. For the beginning practitioner, the scope of the math that forms the foundation of ML is intimidating, and many choose to reassure themselves by observing that modern programming frameworks mean that the ability to implement an algorithm is far removed from the ability to understand its mathematical underpinnings. 

This "top-down" approach, emphasizing results over theory, is both effective and marketable. However, there are certain mathematical foundations of ML that are worth becoming familiar with.

In what follows, I state some of the most important equations a beginner should seek to understand when approaching ML. The contents of this document will lay the groundwork for at least one article on Medium.

\section{Functions and Derivatives}

\subsection{Variables and Function Notation}

$$ y = f(x) $$

\subsection{Equation of a Line}

$$ y = mx + b $$

\subsection{The Power Rule}

$$ \dv{}{x}\left(x^n\right)  = nx^{n-1} $$

\subsection{Finite Differences and Difference Quotients}

$$ \frac{\Delta f(x)}{\Delta x} = \frac{f\left(x+\Delta x\right) - f(x)}{\Delta x} $$
$$ \Delta f(x) = f\left(x+\Delta x\right) - f(x) $$
$$ \nabla f(x) = f(x) - f\left(x - \Delta x\right) $$
$$ \delta f(x) = f\left(x+\frac{1}{2}\Delta x\right) - f\left(x - \frac{1}{2}\Delta x\right) $$

\section{Vectors and Matrices}

$$ \vec{a}, \vec{b} \in \mathbb{R}^n $$

\subsection{Vector Arithmetic}

$$ \vec{a} + \vec{b} = (a_1+b_1, a_2+b_2, \ldots, a_n+b_n) $$
$$ \lambda\vec{a} = (\lambda a_1, \lambda a_2,\ldots, \lambda a_n) $$

\subsection{Vector Magnitude}

$$ |\vec{a}| = \sum_{i=1}^{n}\sqrt{a_i^2} = \sqrt{a_1 + a_2 + \ldots + a_n} $$

\subsection{L1 and L2 Norms}

$$ ||\vec{a}||_1 = \sum_{i=1}^{n} |a_i| $$
$$ ||\vec{a}||_1 = \sum_{i=1}^{n} \sqrt{a_i^2} $$

\subsection{Matrix Multiplication}

$$ \mathbf{A} \in \mathbb{R}^{m \times n}, \mathbf{B} \in \mathbb{R}^{n \times p} $$

$$ \mathbf{A} = 
\begin{pmatrix}
a_{11} & a_{12} & \ldots & a_{1n} \\
a_{21} & a_{22} & \ldots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \ldots & a_{mn}
\end{pmatrix}
$$

$$ \mathbf{AB} \in \mathbb{R}^{m \times p} $$

$$ \mathbf{C} = \mathbf{AB} = 
\begin{pmatrix}
c_{11} & c_{12} & \ldots & c_{1p} \\
c_{21} & c_{22} & \ldots & c_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
c_{m1} & c_{m2} & \ldots & c_{mp}
\end{pmatrix}
$$

$$ c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \ldots + a_{im}b_{mj} = \sum_{k=1}^{m}a_{ik}b_{kj} $$

\subsection{Matrix Inversion}

$$ \mathbf{A} \in \mathbb{R}^{n \times n} $$

$$ \mathbf{A}^{-1}\mathbf{A} = \mathbf{AA}^{-1} = \mathbf{I}_n $$

$$ \mathbf{A}^{-1} \in \mathbb{R}^{n \times n} $$

$$ \mathbf{I} = 
\begin{pmatrix}
1 & 0 & \ldots & 0 \\
0 & 1 & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & 1
\end{pmatrix}
$$

\section{Statistical Relationships and Procedures}

\subsection{Correlation}

$$ \rho_{\mathbf{X}, \mathbf{Y}} = \frac{\mathbf{cov}(\mathbf{X}, \mathbf{Y})}{\sigma_{\mathbf{X}}\sigma_{\mathbf{Y}}} $$

$$ = \frac{E[(\mathbf{X}-\mu_{\mathbf{X}})(\mathbf{Y}-\mu_{\mathbf{Y}})]}{\sigma_{\mathbf{X}}\sigma_{\mathbf{Y}}} $$

$$ r_{xy} = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}
{\sqrt{
\sum_{i=1}^{n}(x_i-\bar{x})^2
\sum_{i=1}^{n}(y_i-\bar{y})^2
}} $$

$$ \bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i $$

\subsection{Principal Components Analysis}

$$ \mathbf{w_{(1)}} = \argmax_{||\mathbf{w}||=1}\left\{ ||\mathbf{Xw}||^2 \right\} = \argmax_{||\mathbf{w}||=1}\left\{ ||\mathbf{w}^T\mathbf{X}^T\mathbf{Xw}||^2 \right\}$$

$$ \mathbf{w_{(1)}}=\argmax\left\{\frac{||\mathbf{w}^T\mathbf{X}^T\mathbf{Xw}||^2}{\mathbf{w}^T\mathbf{w}}\right\}$$

$$ \mathbf{W}_k = \argmax_{||\mathbf{w}||=1}\left\{ ||\mathbf{\hat{X}}_k\mathbf{w}||^2 \right\} $$

$$ \mathbf{\hat{X}}_k = \mathbf{X}-\sum_{s=1}^{k-1}\mathbf{Xw}_{(s)}\mathbf{w}_{(s)}^T $$

\subsubsection{Singular-Value Decomposition (SVD)}

$$ \mathbf{M} \in \mathbb{R}^{m \times n}$$

$$ \mathbf{M} = \mathbf{U\Sigma V}^* $$

\begin{itemize}
  \item The nonzero singular values are the square roots of the nonzero eigenvalues of $\mathbf{M}^*\mathbf{M}$ or $\mathbf{MM}^*$
  \item $\mathbf{\Sigma}$ is a diagonal $(m \times n)$ matrix of non-negative real numbers, known as the \textbf{singular values} of $\mathbf{M}$
  \item The columns of $\mathbf{V}$ (right singular vectors) are eigenvectors of $\mathbf{M}^*\mathbf{M}$
  \item The columns of $\mathbf{U}$ (left singular vectors) are eigenvectors of $\mathbf{MM}^*$
\end{itemize}

\end{document}

