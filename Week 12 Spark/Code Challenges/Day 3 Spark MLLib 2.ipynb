{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b-y9l5SxPLKQ"
   },
   "source": [
    "### Coding Challenge #3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oXX0kIcBNJdq"
   },
   "source": [
    "In this coding challenge, you will work through a couple of scenarios that help you become acquainted with the Spark Mllib package to surface predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RAnKnW3-T6rG"
   },
   "source": [
    "**Question 1**:  We are going to utilize the ML library from Spark (specifically a decision tree model) to predict whether a person gets hired or not based on a select set of attributes/features. The **ask** is to train a Decision Tree model on \"Hiring\" related data using the Spark ML library  and then use the trained model on test data to predict outcomes (**hired** or **not hired**)\n",
    "\n",
    "**Dataset**: https://www.dropbox.com/s/owywl67x4y7ftv8/History_Hires.csv?raw=1 - Download the file and save it to a local folder and then utilize the textfile method of the SparkContext package to read in the file\n",
    "\n",
    "The dataset consists of the following attributes:\n",
    "\n",
    "**1) **Years Experience\n",
    "**2) **Employed?\n",
    "**3)** Previous Employers (i.e. how many previous employers they have had)\n",
    "**4) ** Level of Education (i.e. degrees)\n",
    "**5) ** Top-Tier School\n",
    "**6) ** Interned?\n",
    "**7) ** Hired (i.e. dependent variable)\n",
    "\n",
    "Once the decision tree model is trained, test it against the following 2 test candidates and surface predictions\n",
    "\n",
    "**Test Candidate 1**: \n",
    "\n",
    "The first candidate with 10 years of experience, currently employed,\n",
    "3 previous employers, a BS degree, but from a non-top-tier school where he or she did not do an internship\n",
    "\n",
    "**Test Candidate 2**:\n",
    "\n",
    "The second condidate with 0 years of experience, currently not employed,\n",
    "no previous employers, a BS degree, but from a non-top-tier school where\n",
    "he or she did not do an internship.\n",
    "\n",
    "**Stretch Goal**: \n",
    "\n",
    "Make up a large number of test candidates and populate a \"csv\" file. Read the \"csv\" file and then test the trained model against your test candidates to surface predictions\n",
    "\n",
    "Reference: https://spark.apache.org/docs/2.3.0/mllib-decision-tree.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"CC3\").setMaster(\"local[2]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile('https://uc22357bc171769db5c2d50b65b9.dl.dropboxusercontent.com/cd/0/inline/AJXdOZX3BxXZVGAFZypk6TdnQeDGQex5n28d0Pev9CJde_uJyKDxKhT9qunecHjGEBZIKZJN6GAM8dfsMGgrLxofjN5fjy0DpsSsXgQAJginaKoemAT35zZ0QXI-8lpHKViGjB5YRl766ASgqOl50hJgNKxqqWWqxOkDuOAZIwjgMFDkXXkYAS5cv2D4ZOnB-6c/file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(SparkFiles.get('file'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = data.first()\n",
    "data = data.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda line: line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['10', 'Y', '4', 'BS', 'N', 'N', 'Y'],\n",
       " ['0', 'N', '0', 'BS', 'Y', 'Y', 'Y'],\n",
       " ['7', 'N', '6', 'BS', 'N', 'N', 'N'],\n",
       " ['2', 'Y', '1', 'MS', 'Y', 'N', 'Y'],\n",
       " ['20', 'N', '2', 'PhD', 'Y', 'N', 'N'],\n",
       " ['0', 'N', '0', 'PhD', 'Y', 'Y', 'Y'],\n",
       " ['5', 'Y', '2', 'MS', 'N', 'Y', 'Y'],\n",
       " ['3', 'N', '1', 'BS', 'N', 'Y', 'Y'],\n",
       " ['15', 'Y', '5', 'BS', 'N', 'N', 'Y'],\n",
       " ['0', 'N', '0', 'BS', 'N', 'N', 'N'],\n",
       " ['1', 'N', '1', 'PhD', 'Y', 'N', 'N'],\n",
       " ['4', 'Y', '1', 'BS', 'N', 'Y', 'Y'],\n",
       " ['0', 'N', '0', 'PhD', 'Y', 'N', 'Y']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_no_dict = {'N': 0, 'Y': 1}\n",
    "edu_dict = {'BS': 0, 'MS': 1, 'PhD': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(line, indexer, col):\n",
    "    line[col] = indexer[line[col]]\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# throwing errors if I try to put in for loop\n",
    "data = data.map(lambda line: label_encode(line, yes_no_dict, 1))\n",
    "data = data.map(lambda line: label_encode(line, edu_dict, 3))\n",
    "data = data.map(lambda line: label_encode(line, yes_no_dict, 4))\n",
    "data = data.map(lambda line: label_encode(line, yes_no_dict, 5))\n",
    "data = data.map(lambda line: label_encode(line, yes_no_dict, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_labeledpoint(line):\n",
    "    label = line.pop()    \n",
    "    \n",
    "    for element in line:\n",
    "        try:\n",
    "            int(element)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return LabeledPoint(label, line)\n",
    "\n",
    "data = data.map(line_to_labeledpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTree.trainClassifier(data, numClasses=2, \n",
    "                                     categoricalFeaturesInfo={1:2, 3:3, 4:2, 5:2},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Candidate 1:\n",
    "\n",
    "The first candidate with 10 years of experience, currently employed, 3 previous employers, a BS degree, but from a non-top-tier school where he or she did not do an internship\n",
    "\n",
    "Test Candidate 2:\n",
    "\n",
    "The second condidate with 0 years of experience, currently not employed, no previous employers, a BS degree, but from a non-top-tier school where he or she did not do an internship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = [10, 1, 3, 0, 0, 0]\n",
    "model.predict(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = [0, 0, 0, 0, 0, 0]\n",
    "model.predict(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(SparkFiles.get('file'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_gBf2yRUeDqe"
   },
   "source": [
    "**Question 2**: The ask in this case is to build a Logistic Regression model to decipher whether a body of text is \"Spam\" or \"Ham\". You will leverage the  \"SMSSpamCollection\" file that contains spam and ham messages respectively. You will need to create a feature vector from text data and then train a Logistic Regression model with the entire set of messages (both spam and ham). Once you have trained the model, you will test the model with 2 messages (i.e. one spam message and another ham message) to ascertain how the model categorizes the respective messages (i.e. 1 indicates spam and 0 indicates ham).\n",
    "\n",
    "**Test Message 1 (Spam)**:\n",
    "\n",
    "\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\n",
    "\n",
    "\n",
    "**Test Message 2 (Ham)**:\n",
    "\n",
    "\"I've been searching for the right words to thank you for this breather\"\n",
    "\n",
    "**Dataset**: https://www.dropbox.com/s/z5zm0fxevqvujee/SMSSpamCollection.tsv?raw=1 - Download the file and save it to a local folder and then utilize the textfile method of the SparkContext package to read in the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile('https://uc7ff37af3d623228ac1e3441384.dl.dropboxusercontent.com/cd/0/inline/AJWVbSHDtM1eOSh4hkz0ovY9J81DJozzzjVPrTg7O0uqjhJhrjjIxnm-Liq9IzMlDVbbaXNwUGwm5lnDXY9JCiASulav49bR8pC8d5cUO-SArHcs972RTXPBsuRee54mtkZK_roORzXe9hH2yO0B5z4ivPSfJ4EYJHoQRgQAmI206WkPtg6mUvijHGDZ7w7oAa0/file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(SparkFiles.get('file'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda line: line.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ham',\n",
       "  \"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\"],\n",
       " ['spam',\n",
       "  \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"],\n",
       " ['ham', \"Nah I don't think he goes to usf, he lives around here though\"],\n",
       " ['ham',\n",
       "  'Even my brother is not like to speak with me. They treat me like aids patent.'],\n",
       " ['ham', 'I HAVE A DATE ON SUNDAY WITH WILL!!']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda line: (line[0], line[1].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ham', [\"I've\", 'been', 'searching', 'for', 'the', 'right', 'words', 'to', 'thank', 'you', 'for', 'this', 'breather.', 'I', 'promise', 'i', 'wont', 'take', 'your', 'help', 'for', 'granted', 'and', 'will', 'fulfil', 'my', 'promise.', 'You', 'have', 'been', 'wonderful', 'and', 'a', 'blessing', 'at', 'all', 'times.'])\n"
     ]
    }
   ],
   "source": [
    "print(data.collect()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data.map(lambda x: x[0])\n",
    "documents = data.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_spam = {'ham': 0, 'spam': 1}\n",
    "labels = labels.map(lambda x: ham_spam[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = labels.zip(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " SparseVector(1048576, {1475: 1.0, 70882: 1.0, 151357: 2.0, 154253: 1.0, 163495: 1.0, 173174: 1.0, 231791: 1.0, 235395: 1.0, 238153: 1.0, 241476: 1.0, 250929: 1.0, 270412: 1.0, 276491: 3.0, 463522: 1.0, 479025: 1.0, 486014: 1.0, 488866: 1.0, 494808: 1.0, 550685: 1.0, 578619: 2.0, 622323: 1.0, 648331: 1.0, 702216: 1.0, 706364: 1.0, 724221: 1.0, 789438: 1.0, 837499: 1.0, 910746: 1.0, 935701: 1.0, 990085: 1.0, 1000347: 1.0, 1016101: 1.0, 1031802: 1.0}))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: LabeledPoint(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionWithLBFGS.train(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Message 1 (Spam):\n",
    "\n",
    "\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\n",
    "\n",
    "Test Message 2 (Ham):\n",
    "\n",
    "\"I've been searching for the right words to thank you for this breather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text):\n",
    "    tf = hashingTF.transform(text.split())\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = [\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"]\n",
    "test1 = sc.parallelize(test1)\n",
    "test1 = test1.map(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = [\"I've been searching for the right words to thank you for this breather\"]\n",
    "test2 = sc.parallelize(test2)\n",
    "test2 = test2.map(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test2).collect()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Copy of Spark Coding Challenge #3.ipynb",
   "provenance": [
    {
     "file_id": "1acxVW9K9PiNFU9y-5RtmXbwfk_NiwHW0",
     "timestamp": 1529506869463
    },
    {
     "file_id": "1WkgmnBd3wTIW5DR1jvokNlfnCQNrfR9l",
     "timestamp": 1529506596313
    },
    {
     "file_id": "1oJCI_YSdpHmHwcpGYmnB5c-LBXh50MHM",
     "timestamp": 1528924839108
    },
    {
     "file_id": "15VZIkdnCuuYPwgQYuf958E09sP4lTISb",
     "timestamp": 1528642554887
    },
    {
     "file_id": "1ze4oqASRYRH4sM47GcSEvsIy9HlVxi6p",
     "timestamp": 1528579590698
    },
    {
     "file_id": "1-EV69_DVCzbpMWLiPYHSroeL91624q7j",
     "timestamp": 1528555981083
    },
    {
     "file_id": "1O-exHskPOjgEs1Hrm7utpkHJ-bLmWxbS",
     "timestamp": 1528507363505
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
