{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Tuning Assignment.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1YrBAc3X1e7P56U5HeVwjaVrs5GqDZDT6",
          "timestamp": 1526324841699
        },
        {
          "file_id": "1ec075s7oeOBxW6apQpDb4m7vbTuINdKh",
          "timestamp": 1526324756653
        }
      ],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "j3-9qZ80euzF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part One: K-fold Cross Validation\n",
        "\n",
        "The challenge of training machine learning models is to be able to make accurate predictions on previously unseen real-world data in spite of the fact that we only have a finite training dataset to learn from. \n",
        "\n",
        "One way of validating our model's quality-of-fit and avoiding overfitting/underfitting, is to use the test_train_split method like we did in the code challenge. With this method, the randomly selected test dataset can be used to evaluate how our model performs on data that it has not yet seen in the training process. However, there are downsides to this approach:\n",
        "\n",
        "*   We lose a valuable portion of data that we would prefer to be able to train on to serve as the test dataset. We would prefer to have both the testing and training datasets be as large as possible.\n",
        "*   With small datasets, measures of our model's quality using the test_train_split method often have a high variance. (We saw this behavior when we changed the random seed in the code challenge)\n",
        "\n",
        "We can reduce the severity of both of these drawbacks by using what is called K-fold Cross Validation:\n",
        "\n",
        "[Short Video Explaining K-Fold Cross Validation](https://www.youtube.com/watch?v=TIgfjmp-4BA)\n",
        "\n",
        "[How to Implement K-Fold Cross Validation on the Pima Indians Diabetes dataset](https://machinelearningmastery.com/evaluate-performance-machine-learning-algorithms-python-using-resampling/)"
      ]
    },
    {
      "metadata": {
        "id": "8-xSRguyK0SS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## DO THIS:\n",
        "\n",
        "**1)** Train a logistic regression model on the titanic dataset predicting survivors first using a 20-80% test_train_split and print the accuracy of your model using 5 different random seeds.\n",
        "\n",
        "**2)** Use 5-fold Cross Validation on the titanic dataset. Print out the accuracies from each of the 5 folds of the cross validation, then print the final mean and standard deviation of those cross validation accuracies. How do the accuracies on each of the inidvidual folds compare to the accuracies of the test_train_split approach? Is the variance in accuracies of the cross-validation approach higher or lower than the variance of the test_train_split approach? \n",
        "\n",
        "**3)** Try using 3-fold Cross Validation as well as 10-fold cross validation. How does the number of folds in the cross-validation process affect the outcome? How many folds should be used?\n",
        "\n",
        "---\n",
        "I would give you more boilerplate code here, but I don't want to make it too easy. The articles linked above should be sufficient for this purpose."
      ]
    },
    {
      "metadata": {
        "id": "Wd3B2oWCbUGJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ynECx_nbpUw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following is a script that I've used multiple times to load and preprocess my Titanic data. It encodes the ordered categorical variables as integers, one-hot encodes `Embarked`, fills missing values with the most frequent value for categorical variables and the median for the continuous variables, and drops duplicate rows and redundant columns."
      ]
    },
    {
      "metadata": {
        "id": "kqwcuyA9K6dq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bc117cfb-9feb-490b-9a62-75f098bde8af",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526428608969,
          "user_tz": 420,
          "elapsed": 582,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def fill_mixed_median_mode(dataframe, medians=list()):\n",
        "    \"\"\" Fill missing values with median for specified column, otherwise mode\n",
        "    \n",
        "    Args:\n",
        "        dataframe (pandas.core.frame.DataFrame): rows of observations of features\n",
        "        medians (list): columns to fill missing values with median instead of mode\n",
        "        \n",
        "    Returns:\n",
        "        dataframe with no missing values\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    null = dataframe.isnull().any()\n",
        "    null_cols = list(null[null].index)\n",
        "    \n",
        "    fill = pd.Series([data[c].median() if c in medians else data[c].mode()[0]\n",
        "                     for c in null_cols], index=null_cols)\n",
        "    \n",
        "    dataframe[null_cols] = dataframe[null_cols].fillna(fill)\n",
        "    return dataframe\n",
        "\n",
        "data = sns.load_dataset('titanic')\n",
        "data = data.drop(['alive','adult_male','who','class','embark_town', 'deck'], axis=1)\n",
        "data = data.drop_duplicates()\n",
        "\n",
        "data_f = fill_mixed_median_mode(data, ['age', 'fare'])\n",
        "\n",
        "for label in ['embarked','sex', 'alone']:\n",
        "    data_f[label] = LabelEncoder().fit_transform(data_f[label])\n",
        "\n",
        "embarked_one_hot = OneHotEncoder().fit_transform(data_f[['embarked']]).toarray()\n",
        "embarked = pd.DataFrame(embarked_one_hot, \n",
        "                        columns=['Southampton', 'Cherbourg', 'Queenstown'], \n",
        "                        dtype=np.int64)\n",
        "\n",
        "data_f = data_f.reset_index(drop=True)\n",
        "data_enc = data_f.join([embarked])\n",
        "data_enc = data_enc.drop(['embarked'], axis=1)\n",
        "\n",
        "data_enc.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>pclass</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>alone</th>\n",
              "      <th>Southampton</th>\n",
              "      <th>Cherbourg</th>\n",
              "      <th>Queenstown</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   survived  pclass  sex   age  sibsp  parch     fare  alone  Southampton  \\\n",
              "0         0       3    1  22.0      1      0   7.2500      0            0   \n",
              "1         1       1    0  38.0      1      0  71.2833      0            1   \n",
              "2         1       3    0  26.0      0      0   7.9250      1            0   \n",
              "3         1       1    0  35.0      1      0  53.1000      0            0   \n",
              "4         0       3    1  35.0      0      0   8.0500      1            0   \n",
              "\n",
              "   Cherbourg  Queenstown  \n",
              "0          0           1  \n",
              "1          0           0  \n",
              "2          0           1  \n",
              "3          0           1  \n",
              "4          0           1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "KRWL-0Ssc3DE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Test Split"
      ]
    },
    {
      "metadata": {
        "id": "Fh9AMEpBdByh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "X = data_enc.drop('survived', axis=1)\n",
        "Y = data_enc[['survived']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o7zFWTx1dZ_h",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ce356058-19a8-4973-a229-71147858bc82",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526428610501,
          "user_tz": 420,
          "elapsed": 415,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "seeds = [10, 21, 41, 42, 66]\n",
        "\n",
        "for seed in seeds:\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y.values.ravel(), \n",
        "                                                        test_size=0.2, random_state=seed)\n",
        "    \n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, Y_train)\n",
        "    \n",
        "    accuracy = model.score(X_test, Y_test)\n",
        "    \n",
        "    print('Random Seed: {}, Accuracy: {:.1f}%'.format(seed, accuracy*100))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed: 10, Accuracy: 75.0%\n",
            "Random Seed: 21, Accuracy: 80.1%\n",
            "Random Seed: 41, Accuracy: 78.2%\n",
            "Random Seed: 42, Accuracy: 79.5%\n",
            "Random Seed: 66, Accuracy: 78.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qxRmwDw0c5bl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5-fold Cross Validation"
      ]
    },
    {
      "metadata": {
        "id": "-Ijj-wZ_eiaE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c95bfad8-63c2-4a00-efb0-71eba49b24c4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526428611266,
          "user_tz": 420,
          "elapsed": 521,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=5, random_state=41)\n",
        "kf.get_n_splits(X)\n",
        "\n",
        "validation_accuracy = 0\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
        "    \n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, Y_train)\n",
        "    \n",
        "    accuracy = model.score(X_test, Y_test.values.ravel())\n",
        "    validation_accuracy += accuracy\n",
        "    \n",
        "validation_accuracy /= 5\n",
        "\n",
        "print('Mean Validation Accuracy Across 5 Folds: {:.1f}%'.format(validation_accuracy*100))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Validation Accuracy Across 5 Folds: 76.2%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "q57QHLwhc8KS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Trying Multiple Folds"
      ]
    },
    {
      "metadata": {
        "id": "V58Fd58jgmRq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I will wrap the above code in a function so that it is straightforward to reuse it for differing amounts of folds."
      ]
    },
    {
      "metadata": {
        "id": "Xrtet_Ixgkmp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def kfold_accuracy(X, Y, k=5, random_state=41):\n",
        "    kf = KFold(n_splits=k, random_state=random_state)\n",
        "    kf.get_n_splits(X)\n",
        "\n",
        "    validation_accuracy = 0\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "        Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
        "\n",
        "        model = LogisticRegression()\n",
        "        model.fit(X_train, Y_train)\n",
        "\n",
        "        accuracy = model.score(X_test, Y_test.values.ravel())\n",
        "        validation_accuracy += accuracy\n",
        "\n",
        "    return validation_accuracy / k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GvJ76Y_Hg6-L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "folds = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "accuracies = [kfold_accuracy(X, Y, k) for k in folds]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PF-VXGhAhlFM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "ce2dc2d2-9c05-467f-fb81-a5e7a0a25e8c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526428613667,
          "user_tz": 420,
          "elapsed": 469,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(folds, accuracies)\n",
        "ax.set(title='Average Validation Accuracy', \n",
        "       xlabel='Cross Validation Folds', \n",
        "       ylabel='Accuracy');"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAFnCAYAAABO7YvUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngynoRUREDPb/AX0uxCnVBGsPAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f4c7aa61090>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "VB4cLu7yity8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There is a noticeable jump in validation accuracy when going from using 2 to 3 folds. As the number of folds is increased, the model is trained on a larger proportion of the data, and so should benefit from this up to a certain point.\n",
        "\n",
        "The other trend is that as the folds increase, the proportion of the data used for validation decreases, and thus becomes less accurate as a statistical estimate of the model's ability to generalize to unseen data.\n",
        "\n",
        "One way to determine the number of folds to use is to also include the variance of the validation accuracy. This can then be used to understand how reliable of an estimate the average validation accuracy across folds then is. One can then construct selection criteria based off of this. For example, selecting a value for k such that no greater value has an estimate of the accuracy which is one standard deviation above the accuracy at k, using the standard deviation for the higher value.\n",
        "\n",
        "Of course, the choice to use 1 standard deviation as a sort of confidence interval is arbitrary. Better, more motivated choices likely exist."
      ]
    },
    {
      "metadata": {
        "id": "HhW-kEbzjkIG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def kfold_accuracy_with_std(X, Y, k=5, random_state=41):\n",
        "    kf = KFold(n_splits=k, random_state=random_state)\n",
        "    kf.get_n_splits(X)\n",
        "\n",
        "    accuracies = []\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "        Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
        "\n",
        "        model = LogisticRegression()\n",
        "        model.fit(X_train, Y_train)\n",
        "\n",
        "        accuracy = model.score(X_test, Y_test.values.ravel())\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "    return np.mean(accuracies), np.std(accuracies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oGMSS23rj2sC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "accuracies = []\n",
        "stds = []\n",
        "for k in folds:\n",
        "    acc, std = kfold_accuracy_with_std(X, Y, k)\n",
        "    accuracies.append(acc)\n",
        "    stds.append(std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "trvuq5YakZS4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "b18d1625-1b04-433a-c4aa-6b981cb30710",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526428615810,
          "user_tz": 420,
          "elapsed": 386,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.errorbar(folds, accuracies, yerr=stds, fmt='o')\n",
        "ax.set(title='Average Validation Accuracy', \n",
        "       xlabel='Cross Validation Folds', \n",
        "       ylabel='Accuracy');"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngCEIdQAADEGoAwBgCEIdAABD/H/WM45w34GCDQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f4c7892c3d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "48LtL4KFl2Me",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using the example selection criterion desribed above, I would select 3 folds, since one standard deviation below its accuracy estimate is still greater than the acuracy estimate for 2 folds."
      ]
    },
    {
      "metadata": {
        "id": "1qRSmAPZo0Qr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part Two: Hyperparameter Tuning\n",
        "\n",
        "An important technique for improving the accuracy of a machine learning model is to undertake a process known as Hyperparameter Tuning or Hyperparameter Optimization. In order to understand this process, we first need to understand the difference between a model parameter and a model hyperparameter. \n",
        "\n",
        "### What is a model parameter?\n",
        "\n",
        "A model parameter is a value that is generated by fitting our model to training data and is key to generating predictions with that model. They are **internal** to our model and we often are trying to estimate them as best as possible when we train the algorithm. \n",
        "\n",
        "For example, the parameters of a linear regression model would be its intercept value as well as the coefficient values on each of the X variables. Estimates of these crucial values (parameters) are obtained by fitting to the training data, perfectly define the model, are internal to the model, and are key to generating predictions. They are model parameters in every sense. \n",
        "\n",
        "### What is a model hyperparameter?\n",
        "\n",
        "Hyperparameters are values that are key to how well our algorithm runs, yet are **external** to our model and cannot be estimated from the training process. They are more like settings for our algorithm which must be designated before it is run and impact its performance. Here is some further reading:\n",
        "\n",
        "[Hyperparamters explanation on Quora](https://www.quora.com/What-are-hyperparameters-in-machine-learning)\n",
        "\n",
        "[Jason Brownlee Article on the difference between Parameters and Hyperparameters](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)\n",
        "\n",
        "### How do we find the best hyperparameters?\n",
        "\n",
        "Since we can't learn the best hyperparameters for our model from the data, we essentially just pick values and see which ones lead to the highest accuracy. This can be a tedious and complex process especially for certain models like neural networks which can have dozens of hyperparameters. We will get you familiar with the process using a more simple logistic regression model. \n",
        "\n",
        "### How do you know what hyperparameters exist for your particular model? \n",
        "\n",
        "Most models/libraries have default hyperparameters that will be used if we don't specify them. In the model selection process you might try out multiple models on a dataset and see which one gets you the highest out-of-the-box performance, (using the default hyperparameters) and then pick a couple of the highest performing algorithms and attempt hyperparameter tuning on them to compare how different models benefit from this process. Once you have narrowed down the models that you would like to tune, a quick google search can tell you what hyperparameters exist for that algorithm. \n",
        "\n",
        "Often you can learn about potential hyperparameters by looking at the documentation for a given algorithm in a library, here's the documentation for sklearn's logistic regression, see if you can spot the hyperparameters:\n",
        "\n",
        "[scikit-learn logistic regression docs](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "wADqKJAgSqgI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## DO THIS: \n",
        "\n",
        "Lets hyperparameter tune our **titanic** predictions using 5-fold cross validation to compare the accuracy of our tuned models. \n",
        "\n",
        "### Manual Hyperparameter Tuning:\n",
        "\n",
        "For our assignment today we are going to tune the 'C value' also known as the 'regularization strength' of our logistic regression as well as 'penalty' of our logistic regression algorithm.\n",
        "\n",
        "Read up on the regularlization strength and penalty of a logistic regression function. What might be some good values to test out? Hint: Look at the parameter definitions on the sci-kit learn logistic regression documentation. \n",
        "\n",
        "[scikit-learn logistic regression docs](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "\n",
        "[Regularization in Logistic Regression](https://www.kdnuggets.com/2016/06/regularization-logistic-regression.html)\n",
        "\n",
        "Fit your model 5 different times using 5 different C values of your choosing. Which value gives the highest accuracy? \n",
        "\n",
        "There are only two penalty values that we can use. Evaluate the model two more times using each penalty once. Which penalty gives the highest accuracy?"
      ]
    },
    {
      "metadata": {
        "id": "UJujhJnetnr3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88798a89-f7e3-4803-9c41-00c01a103c31",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526428616509,
          "user_tz": 420,
          "elapsed": 368,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# The sample code below uses the Pima Indans Diabetes Dataset. \n",
        "# Here we are setting the C value hyperparameter to 1 and the penalty hyperparameter to \"l1\". \n",
        "# You can designate your hyperparameters in a similar fashion.\n",
        "\n",
        "import pandas\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = pandas.read_csv(url, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "num_instances = len(X)\n",
        "seed = 7\n",
        "kfold = model_selection.KFold(n_splits=5, random_state=seed)\n",
        "model = LogisticRegression(C=1, penalty='l1') ##### This is the important line\n",
        "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.75974026 0.72727273 0.76623377 0.83006536 0.76470588]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CrwljGvubkpv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1) Grid-Search Hyperparameter Tuning:\n",
        "\n",
        "Imagine that your algorithm has 12 different potential hyperparameters and each them can take on 5 different values. Lets say that it takes your laptop 4 seconds to fit each fold of cross validation. For each 5-fold cross-validation it would then take 20 seconds to fit your model and get an accuracy reading reported back. Now imagine that you want to test every possible combination of hyperparameters on your algorithm to get the absolute highest accuracy. You can see how this might become exceedingly tedious and time-consuming to perform by hand. Some hyperparameters (like the C value) have much more than 5 potential values, making hyperparameter tuning a huge task. \n",
        "\n",
        "It is for this reason that more advanced optimization techniques exist, one of which we will be exploring today called GridSearch.\n",
        "\n",
        "### What does GridSearch do?\n",
        "\n",
        "GridSearch takes a dictionary of all of the different hyperparameters that you want to test, and then feeds all of the different combinations through the algorithm for you and then reports back to you which one had the highest accuracy. Pretty slick right? \n",
        "\n",
        "Here is some boilerplate code you can reference to create your implementations:\n",
        "\n",
        "[Chris Albon Logistic Regression sklearn Hyperparameter Tuning with GridSearch](https://chrisalbon.com/machine_learning/model_selection/hyperparameter_tuning_using_grid_search/)"
      ]
    },
    {
      "metadata": {
        "id": "dCZLjm7JjTwh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# These import statements might be useful to you. \n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sb4F7GlnjIvl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Create logistic regression object\n",
        "model = LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nrrfWSJOey0J",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Create a list of all of the different penalty values that you want to test and save them to a variable called 'penalty'\n",
        "penalty = ['l1', 'l2']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-m1rVmNwfbSH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Create a list of all of the different C values that you want to test and save them to a variable called 'C'\n",
        "C = np.logspace(-1, 3, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hy-B_Wm3fz-4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0d5e7bb-2f3e-432d-fb1c-f1f1c3927f64",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526428621448,
          "user_tz": 420,
          "elapsed": 518,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Now that you have two lists each holding the different values that you want test, use the dict() function to combine them into a dictionary. \n",
        "# Save your new dictionary to the variable 'hyperparameters'\n",
        "# Print out the dictionary if you're curious as to what it euds up looking like.\n",
        "\n",
        "hyperparameters = dict(C=C, penalty=penalty)\n",
        "print(hyperparameters)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'penalty': ['l1', 'l2'], 'C': array([1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v619p6o4j9-3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Fit your model using gridsearch\n",
        "\n",
        "clf = GridSearchCV(model, hyperparameters, cv=5, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3aUuCQH4gdBc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5840888-e84a-4bee-8cec-718360a28ec5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526428623146,
          "user_tz": 420,
          "elapsed": 832,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Print the best penalty and C value from best_model.best_estimator_.get_params()\n",
        "\n",
        "best_model = clf.fit(X, Y)\n",
        "best_params = best_model.best_estimator_.get_params()\n",
        "\n",
        "print('Best penalty: {}, Best C: {}'.format(best_params['penalty'], best_params['C']))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best penalty: l2, Best C: 100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IqG4tPaQkKgz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "1352091e-7852-4e92-e53e-2015ccc8f58d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526428623595,
          "user_tz": 420,
          "elapsed": 363,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Print out all of the different combinations of your grid search values and their corresponding accuracies.\n",
        "# https://stackoverflow.com/questions/22155953/how-to-print-out-an-accuracy-score-for-each-combination-within-gridsearch\n",
        "\n",
        "params, mean, std = [clf.cv_results_[key] for key in ['params', 'mean_test_score', 'std_test_score']]\n",
        "pty = [p['penalty'] for p in params]\n",
        "c = [p['C'] for p in params]\n",
        "\n",
        "gridsearch = pd.DataFrame([pd.Series(x) for x in [pty, c, mean, std]]).T\n",
        "gridsearch.columns = ['Penalty', 'C', 'Accuracy: Mean', 'Accuracy: Standard Deviation']\n",
        "\n",
        "gridsearch"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Penalty</th>\n",
              "      <th>C</th>\n",
              "      <th>Accuracy: Mean</th>\n",
              "      <th>Accuracy: Standard Deviation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>l1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.759115</td>\n",
              "      <td>0.022952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>l2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.727865</td>\n",
              "      <td>0.0170129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>l1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.766927</td>\n",
              "      <td>0.0235538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>l2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.768229</td>\n",
              "      <td>0.0186303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>l1</td>\n",
              "      <td>10</td>\n",
              "      <td>0.770833</td>\n",
              "      <td>0.0221773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>l2</td>\n",
              "      <td>10</td>\n",
              "      <td>0.766927</td>\n",
              "      <td>0.0232765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>l1</td>\n",
              "      <td>100</td>\n",
              "      <td>0.770833</td>\n",
              "      <td>0.0247179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>l2</td>\n",
              "      <td>100</td>\n",
              "      <td>0.772135</td>\n",
              "      <td>0.0245323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>l1</td>\n",
              "      <td>1000</td>\n",
              "      <td>0.770833</td>\n",
              "      <td>0.0247179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>l2</td>\n",
              "      <td>1000</td>\n",
              "      <td>0.770833</td>\n",
              "      <td>0.0247179</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Penalty     C Accuracy: Mean Accuracy: Standard Deviation\n",
              "0      l1   0.1       0.759115                     0.022952\n",
              "1      l2   0.1       0.727865                    0.0170129\n",
              "2      l1     1       0.766927                    0.0235538\n",
              "3      l2     1       0.768229                    0.0186303\n",
              "4      l1    10       0.770833                    0.0221773\n",
              "5      l2    10       0.766927                    0.0232765\n",
              "6      l1   100       0.770833                    0.0247179\n",
              "7      l2   100       0.772135                    0.0245323\n",
              "8      l1  1000       0.770833                    0.0247179\n",
              "9      l2  1000       0.770833                    0.0247179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "IwOTSHaUkd8J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What hyperparameters give you the highest accuracy? Keep on testing diferent values and report the hyperparameters that give you the highest accuracy."
      ]
    },
    {
      "metadata": {
        "id": "LJVa3cMmwV5b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since performing a grid search with different orders of magnitude for C produced 100 as the best value, I will search more exhaustively around this."
      ]
    },
    {
      "metadata": {
        "id": "PrFdrTxvvgG4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "C = np.linspace(1, 150)\n",
        "hyperparameters = dict(C=C, penalty=penalty)\n",
        "clf = GridSearchCV(model, hyperparameters, cv=5, verbose=0)\n",
        "best_model = clf.fit(X, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pywcvbuavzQP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cacafa10-a540-443d-ace5-f3aac343fd52",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526428889509,
          "user_tz": 420,
          "elapsed": 733,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "best_params = best_model.best_estimator_.get_params()\n",
        "print('Best penalty: {}, Best C: {}'.format(best_params['penalty'], best_params['C']))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best penalty: l1, Best C: 19.2448979592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H-dhzwBkQJ5Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The C value found is still quite high, indicating that regularization hurts the performance of the model. This is unsurprising, since logistic regression as a model has very low variance, and penalizing its parameters values will lead to underfitting."
      ]
    },
    {
      "metadata": {
        "id": "r-ctaux7qOzv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Stretch Goals:\n",
        "\n",
        "Explore more advanced automated approaches to hyperparameter tuning. Try and implemenet a random search approach: \n",
        "\n",
        "[Random Search Hyperparameter Tuning](https://machinelearningmastery.com/how-to-tune-algorithm-parameters-with-scikit-learn/)\n",
        "\n",
        "Then try a Bayesian Optimization Approach:\n",
        "\n",
        "[Bayesian Optimization](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/)\n",
        "\n",
        "[scikit-optimize](https://scikit-optimize.github.io/notebooks/bayesian-optimization.html)\n",
        "\n",
        "[optunity](http://optunity.readthedocs.io/en/latest/notebooks/notebooks/sklearn-automated-classification.html)\n",
        "\n",
        "You could also try writing a blog post to show how well you understand Cross Validation or Hyperparameter Tuning, both are key concepts to practicing machine learning and would be valuable to demonstrate proficency in.\n"
      ]
    },
    {
      "metadata": {
        "id": "rRwas_XW0RUX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Random Search Hyperparameter Tuning"
      ]
    },
    {
      "metadata": {
        "id": "wn5pW4GfxyOW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y2cjeOCdyx2Q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "C = np.logspace(-1, 3, 1000)\n",
        "penalty = ['l1', 'l2']\n",
        "hyperparameters = dict(C=C, penalty=penalty)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kRTKS_qgzBU0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r73_j51KzRDG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8cc29e9-083c-449d-aedb-6ef5aa67eb38",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526429003907,
          "user_tz": 420,
          "elapsed": 2824,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "clf = RandomizedSearchCV(model, hyperparameters, n_iter=100, random_state=41)\n",
        "clf.fit(X, Y)\n",
        "print(clf.best_params_)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'penalty': 'l2', 'C': 1.9828839491270713}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CHm1_gTCQjlA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Random search finds a much lower C value. This is potentially due to the fact that the distribution created by `np.logspace` is skewed. I will repeat the analysis with `np.linspace`."
      ]
    },
    {
      "metadata": {
        "id": "k3Qt1quAQueH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b46a0630-42c3-44df-d623-ef6fa8799e85",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526429081105,
          "user_tz": 420,
          "elapsed": 3057,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "C = np.linspace(1, 1000)\n",
        "penalty = ['l1', 'l2']\n",
        "hyperparameters = dict(C=C, penalty=penalty)\n",
        "\n",
        "clf = RandomizedSearchCV(model, hyperparameters, n_iter=100, random_state=41)\n",
        "clf.fit(X, Y)\n",
        "print(clf.best_params_)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'penalty': 'l1', 'C': 21.387755102040817}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_v1GEqkUQ4FR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Sure enough, this produces results more consistent with what was obtained with the grid search. This demonstrates the fact that a random search samples from the distributions provided, instead of exhaustively searching through their range. Thus, the shape and skew of the distributions influence the result."
      ]
    },
    {
      "metadata": {
        "id": "-n8NTMnl0VS0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bayesian Optimization"
      ]
    },
    {
      "metadata": {
        "id": "mPIqXjTgLkZW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://github.com/thuijskens/bayesian-optimization/blob/master/python/gp.py\n",
        "\n",
        "import sklearn.gaussian_process as gp\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "def expected_improvement(x, gaussian_process, evaluated_loss, greater_is_better=False, n_params=1):\n",
        "    \"\"\" expected_improvement\n",
        "    Expected improvement acquisition function.\n",
        "    Arguments:\n",
        "    ----------\n",
        "        x: array-like, shape = [n_samples, n_hyperparams]\n",
        "            The point for which the expected improvement needs to be computed.\n",
        "        gaussian_process: GaussianProcessRegressor object.\n",
        "            Gaussian process trained on previously evaluated hyperparameters.\n",
        "        evaluated_loss: Numpy array.\n",
        "            Numpy array that contains the values off the loss function for the previously\n",
        "            evaluated hyperparameters.\n",
        "        greater_is_better: Boolean.\n",
        "            Boolean flag that indicates whether the loss function is to be maximised or minimised.\n",
        "        n_params: int.\n",
        "            Dimension of the hyperparameter space.\n",
        "    \"\"\"\n",
        "\n",
        "    x_to_predict = x.reshape(-1, n_params)\n",
        "\n",
        "    mu, sigma = gaussian_process.predict(x_to_predict, return_std=True)\n",
        "\n",
        "    if greater_is_better:\n",
        "        loss_optimum = np.max(evaluated_loss)\n",
        "    else:\n",
        "        loss_optimum = np.min(evaluated_loss)\n",
        "\n",
        "    scaling_factor = (-1) ** (not greater_is_better)\n",
        "\n",
        "    # In case sigma equals zero\n",
        "    with np.errstate(divide='ignore'):\n",
        "        Z = scaling_factor * (mu - loss_optimum) / sigma\n",
        "        expected_improvement = scaling_factor * (mu - loss_optimum) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
        "        expected_improvement[sigma == 0.0] == 0.0\n",
        "\n",
        "    return -1 * expected_improvement\n",
        "\n",
        "\n",
        "def sample_next_hyperparameter(acquisition_func, gaussian_process, evaluated_loss, greater_is_better=False,\n",
        "                               bounds=(0, 10), n_restarts=25):\n",
        "    \"\"\" sample_next_hyperparameter\n",
        "    Proposes the next hyperparameter to sample the loss function for.\n",
        "    Arguments:\n",
        "    ----------\n",
        "        acquisition_func: function.\n",
        "            Acquisition function to optimise.\n",
        "        gaussian_process: GaussianProcessRegressor object.\n",
        "            Gaussian process trained on previously evaluated hyperparameters.\n",
        "        evaluated_loss: array-like, shape = [n_obs,]\n",
        "            Numpy array that contains the values off the loss function for the previously\n",
        "            evaluated hyperparameters.\n",
        "        greater_is_better: Boolean.\n",
        "            Boolean flag that indicates whether the loss function is to be maximised or minimised.\n",
        "        bounds: Tuple.\n",
        "            Bounds for the L-BFGS optimiser.\n",
        "        n_restarts: integer.\n",
        "            Number of times to run the minimiser with different starting points.\n",
        "    \"\"\"\n",
        "    best_x = None\n",
        "    best_acquisition_value = 1\n",
        "    n_params = bounds.shape[0]\n",
        "\n",
        "    for starting_point in np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, n_params)):\n",
        "\n",
        "        res = minimize(fun=acquisition_func,\n",
        "                       x0=starting_point.reshape(1, -1),\n",
        "                       bounds=bounds,\n",
        "                       method='L-BFGS-B',\n",
        "                       args=(gaussian_process, evaluated_loss, greater_is_better, n_params))\n",
        "\n",
        "        if res.fun < best_acquisition_value:\n",
        "            best_acquisition_value = res.fun\n",
        "            best_x = res.x\n",
        "\n",
        "    return best_x\n",
        "\n",
        "\n",
        "def bayesian_optimisation(n_iters, sample_loss, bounds, x0=None, n_pre_samples=10,\n",
        "                          gp_params=None, random_search=False, alpha=1e-5, epsilon=1e-7):\n",
        "    \"\"\" bayesian_optimisation\n",
        "    Uses Gaussian Processes to optimise the loss function `sample_loss`.\n",
        "    Arguments:\n",
        "    ----------\n",
        "        n_iters: integer.\n",
        "            Number of iterations to run the search algorithm.\n",
        "        sample_loss: function.\n",
        "            Function to be optimised.\n",
        "        bounds: array-like, shape = [n_params, 2].\n",
        "            Lower and upper bounds on the parameters of the function `sample_loss`.\n",
        "        x0: array-like, shape = [n_pre_samples, n_params].\n",
        "            Array of initial points to sample the loss function for. If None, randomly\n",
        "            samples from the loss function.\n",
        "        n_pre_samples: integer.\n",
        "            If x0 is None, samples `n_pre_samples` initial points from the loss function.\n",
        "        gp_params: dictionary.\n",
        "            Dictionary of parameters to pass on to the underlying Gaussian Process.\n",
        "        random_search: integer.\n",
        "            Flag that indicates whether to perform random search or L-BFGS-B optimisation\n",
        "            over the acquisition function.\n",
        "        alpha: double.\n",
        "            Variance of the error term of the GP.\n",
        "        epsilon: double.\n",
        "            Precision tolerance for floats.\n",
        "    \"\"\"\n",
        "\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "\n",
        "    n_params = bounds.shape[0]\n",
        "\n",
        "    if x0 is None:\n",
        "        for params in np.random.uniform(bounds[:, 0], bounds[:, 1], (n_pre_samples, bounds.shape[0])):\n",
        "            x_list.append(params)\n",
        "            y_list.append(sample_loss(params))\n",
        "    else:\n",
        "        for params in x0:\n",
        "            x_list.append(params)\n",
        "            y_list.append(sample_loss(params))\n",
        "\n",
        "    xp = np.array(x_list)\n",
        "    yp = np.array(y_list)\n",
        "\n",
        "    # Create the GP\n",
        "    if gp_params is not None:\n",
        "        model = gp.GaussianProcessRegressor(**gp_params)\n",
        "    else:\n",
        "        kernel = gp.kernels.Matern()\n",
        "        model = gp.GaussianProcessRegressor(kernel=kernel,\n",
        "                                            alpha=alpha,\n",
        "                                            n_restarts_optimizer=10,\n",
        "                                            normalize_y=True)\n",
        "\n",
        "    for n in range(n_iters):\n",
        "\n",
        "        model.fit(xp, yp)\n",
        "\n",
        "        # Sample next hyperparameter\n",
        "        if random_search:\n",
        "            x_random = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(random_search, n_params))\n",
        "            ei = -1 * expected_improvement(x_random, model, yp, greater_is_better=True, n_params=n_params)\n",
        "            next_sample = x_random[np.argmax(ei), :]\n",
        "        else:\n",
        "            next_sample = sample_next_hyperparameter(expected_improvement, model, yp, greater_is_better=True, bounds=bounds, n_restarts=100)\n",
        "\n",
        "        # Duplicates will break the GP. In case of a duplicate, we will randomly sample a next query point.\n",
        "        if np.any(np.abs(next_sample - xp) <= epsilon):\n",
        "            next_sample = np.random.uniform(bounds[:, 0], bounds[:, 1], bounds.shape[0])\n",
        "\n",
        "        # Sample loss for new set of parameters\n",
        "        cv_score = sample_loss(next_sample)\n",
        "\n",
        "        # Update lists\n",
        "        x_list.append(next_sample)\n",
        "        y_list.append(cv_score)\n",
        "\n",
        "        # Update xp and yp\n",
        "        xp = np.array(x_list)\n",
        "        yp = np.array(y_list)\n",
        "\n",
        "    return xp, yp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aLfgie5rNwDu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Because `penalty` is not a hyperparameter with continuous values, I will have to associate intervals with each individual type of penalty. The value for penalty found through Bayesian optimization can then be interpreted as a measure of certainty that the penalty is `l2`. The discontinuity introduced may hurt the performance of the optimizer, but since `C` is really the hyperparameter of interest, I am willing to accept this."
      ]
    },
    {
      "metadata": {
        "id": "rvoYGiVqL4iE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def sample_loss(params):\n",
        "    C = params[0]\n",
        "    penalty = 'l1' if params[1] < (bounds[1,1]//2) else 'l2'\n",
        "    \n",
        "    model = LogisticRegression(C=C, penalty=penalty)\n",
        "    \n",
        "    return cross_val_score(model,\n",
        "                           X=X,\n",
        "                           y=Y,\n",
        "                           cv=5).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9xwnKEh9DzIo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d351453f-5641-4d3a-b169-046e89ae6c40",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526429205300,
          "user_tz": 420,
          "elapsed": 30007,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "n_iters = 10\n",
        "bounds = np.array([[0, 10], [0, 1]])\n",
        "hyperparameters, scores = bayesian_optimisation(n_iters, sample_loss, bounds)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([1.39546981e-05]), 'nit': 7, 'funcalls': 51}\n",
            "  \" state: %s\" % convergence_dict)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "o_vxT8Y8MtWM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a8286af9-ee8a-4960-d82c-c9744406143a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526429217253,
          "user_tz": 420,
          "elapsed": 457,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "best_ix = np.argmax(scores)\n",
        "print(best_ix)\n",
        "best_C = hyperparameters[best_ix,0]\n",
        "best_penalty = 'l1' if hyperparameters[best_ix,1] < 0.5 else 'l2'\n",
        "print('Best penalty: {}, Best C: {}'.format(best_penalty, best_C))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "Best penalty: l1, Best C: 5.71889177486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z4lL74TiRg4n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The best hyperparameters were not found over the course of the 10 Bayesian optimization iterations, but were from the 10 initial samples. I will try searching only for a value of C, with `l1` as the penalty."
      ]
    },
    {
      "metadata": {
        "id": "PrC9dOFkRtFZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def sample_loss(params):\n",
        "    C = params[0]\n",
        "    \n",
        "    model = LogisticRegression(C=C, penalty='l1')\n",
        "    \n",
        "    return cross_val_score(model,\n",
        "                           X=X,\n",
        "                           y=Y,\n",
        "                           cv=5).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yrZh0TSAR4b6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d7c4c94e-714e-44b5-ebf4-b9158cf108c8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526429653817,
          "user_tz": 420,
          "elapsed": 25285,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "n_iters = 10\n",
        "bounds = np.array([[0, 20]])\n",
        "hyperparameters, scores = bayesian_optimisation(n_iters, sample_loss, bounds, n_pre_samples=5)\n",
        "\n",
        "best_ix = np.argmax(scores)\n",
        "print(best_ix)\n",
        "best_C = hyperparameters[best_ix,0]\n",
        "print('Best C: {}'.format(best_C))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "Best C: 19.316149877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z5KXqLyITGJ3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bayesian optimization finds the best hyperparameter on its first iteration. This indicates that the presamples allowed for an accurate estimate of the loss as a function of the hyperparameters, but that taking further samples biased this estimate towards the neighborhood of these new samples."
      ]
    }
  ]
}