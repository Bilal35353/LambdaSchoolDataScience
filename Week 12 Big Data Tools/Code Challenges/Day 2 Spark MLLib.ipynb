{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b-y9l5SxPLKQ"
   },
   "source": [
    "### Coding Challenge #2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z4NOP7B1W3vc"
   },
   "source": [
    "**Question 1:** This question is meant to provide you with exposure to Spark MLlib data types (i.e. specifically LabelPoint and Dense Vectors)\n",
    "\n",
    "**Dataset**: https://www.dropbox.com/s/cv8kpsqsgxzw5ar/Spiders.csv?raw=1\n",
    "\n",
    "In 2006, Japanese researchers conducted a study to uncover the presence/absense of an endangered burrowing spider based on the size of the grain. The dataset is representative of some of the research they undertook. If you are interested in reviewing the paper, it can be accessed via this link: \n",
    "https://www.jstage.jst.go.jp/article/asjaa/55/2/55_2_79/_pdf\n",
    "\n",
    "**ASK:**\n",
    "\n",
    "**Step 1:** Import the requisite packages\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "**Step 2: ** Read in the \"Spiders.csv\" file\n",
    "\n",
    "**Step 3:** Ignore the header row\n",
    "\n",
    "**Step 4: **Create a RDD of LabeledPoints with the presence or absence of spiders being the label and the value is a dense vector of the grain size\n",
    "\n",
    "**Step 5: ** Convert the RDD into a list/collection and output the list of LabelPoints\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"CC2\").setMaster(\"local[2]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "sc.addFile('https://uc08fd544110d6fbed318ef426e7.dl.dropboxusercontent.com/cd/0/inline/AJR66J7T_NONvcHsz_wksCUeVZMWiOFJy61-hl8ZEZjxBYpDw6RiDLEVBw0UWHDJ2ROYoCSDO5Pjlnd3q45hOUIfOmRP2Vk8iL2fuOcir-oweADIEUhi-x_nVa_Qkdqrk2Qg2NXphK0Qsb-RlqtmxvWUoSuFdAUj6e9Ptfynekt4IxKm5UwS7TU7r0QmCbpTIaU/file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "                                  .option(\"inferSchema\", \"true\")\\\n",
    "                                  .load(SparkFiles.get('file'))\n",
    "\n",
    "# (Step 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|Grain Size (mm)| Spider|\n",
      "+---------------+-------+\n",
      "|          0.245| Absent|\n",
      "|          0.247| Absent|\n",
      "|          0.285|Present|\n",
      "|          0.299|Present|\n",
      "|          0.327|Present|\n",
      "+---------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "features = df.rdd.keys()\n",
    "labels = df.rdd.values().map(lambda x: 0 if x=='Absent' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = labels.zip(features).map(lambda x: LabeledPoint(x[0], Vectors.dense(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.245]),\n",
       " LabeledPoint(0.0, [0.247]),\n",
       " LabeledPoint(1.0, [0.285]),\n",
       " LabeledPoint(1.0, [0.299]),\n",
       " LabeledPoint(1.0, [0.327]),\n",
       " LabeledPoint(1.0, [0.347]),\n",
       " LabeledPoint(0.0, [0.356]),\n",
       " LabeledPoint(1.0, [0.36]),\n",
       " LabeledPoint(0.0, [0.363]),\n",
       " LabeledPoint(1.0, [0.364])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5\n",
    "data.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(SparkFiles.get('file'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oXX0kIcBNJdq"
   },
   "source": [
    "**Question 2**:\n",
    "\n",
    "In this question, you are given the size of houses and associated prices and the **ask** is to predict the price of a house for a given square footage.\n",
    "\n",
    "Here is the snapshot of the dataset that contains the size of houses and the associated prices in the city of Los Gatos (where Netflix is headquartered):\n",
    "\n",
    "![alt text](https://www.dropbox.com/s/2woxl7v5t6i3g5f/HomePrices.JPG?raw=1)\n",
    "\n",
    "**ASK**:\n",
    "\n",
    "**Step 1**: Import the requisite packages\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "\n",
    "**Step 2:** Create a LabeledPoint data type which includes the price of the house as the label and a dense vector of home sizes\n",
    "\n",
    "***Reference:*** https://spark.apache.org/docs/1.2.1/mllib-data-types.html\n",
    "\n",
    "**Step 3:** Create a RDD of the LabelPoint constructed in setp 2 (*Hint*: Utilize the parallelize method of the *SparkContext* object since it ensures that the elements of the RDD can be operated in parallel)\n",
    "\n",
    "**Step 4:** Train a LinearRegressionWithSGD model with  the num of iterations at 100 and a stepSize of 0.0000006\n",
    "\n",
    "**Reference: ** https://spark.apache.org/docs/2.3.0/mllib-linear-methods.html\n",
    "\n",
    "**Step 5:** Predict the price for a house with **2,600** sq ft\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I cannot find a raw data source, so I will use the housing data from __Question 3__ here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile('https://ucf1a7524b617cec1a3482991aba.dl.dropboxusercontent.com/cd/0/inline/AJThx2ikvEC68ehcl0FTjQDaPzIG-PIB7TX2pw42UCQV7oYnhvxEsQKbuTt-tBbd_9XlXOCkuC7U9EX-28phZ7lHtYC2qmFaUEy1y4FSVTh4UQ1i8EHHOqrlBseqfLAIs86sd10nzj5qQoZecxtUbz3rEnUnAs_gtMZimS5Q021HtENLx1KhE9PdDvnNcScr6ZQ/file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "house = sc.textFile(SparkFiles.get('file')).map(lambda line: line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['12839', '2405'],\n",
       " ['10000', '2200'],\n",
       " ['8040', '1400'],\n",
       " ['13104', '1800'],\n",
       " ['10000', '2351']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 (and Step 3 since it produces an RDD)\n",
    "house_data = house.map(lambda x: LabeledPoint(float(x[1]), Vectors.dense(x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(house_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "model = LinearRegressionWithSGD.train(house_data, iterations=100, step=0.0000006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.8779505413723687e+174"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5\n",
    "model.predict(array([2600]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is a massive number, indicating divergence during training (exploding gradients). I will try dramaticly reducing the step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190.80536403423054"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegressionWithSGD.train(house_data, iterations=200, step=0.00000001)\n",
    "model.predict(array([2600]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xOWXkrKLwC-M"
   },
   "source": [
    "In **Question 3**, you are given the lot size of houses and the assocated prices in the city of Saratoga (cloe to the Netflix headquarters) and the ask is to uncover 4 clusters (**k = 4**)  based on the lot size and the price.\n",
    "\n",
    "Here is the snapshot of a subset of the dataset that contains the size of houses and the associated prices in the city of Saratoga:\n",
    "\n",
    "![alt text](https://www.dropbox.com/s/h8yyl0creyi11wg/HomePrices_COS.JPG?raw=1)\n",
    "\n",
    "**Source: ** https://www.neighborhoodscout.com/ca/saratoga/real-estate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E0dTXl5WWJ_p"
   },
   "source": [
    "**Question 2: Ask**\n",
    "\n",
    "**Step 1:** Import the relevant packages\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "**Step 2:** Initialize the Spark Context; the starting point/root of every Spark Application \n",
    "\n",
    "**Step 3:** Load the data into a RDD\n",
    "\n",
    "***Dataset***: https://www.dropbox.com/s/njtjw2272kwk0au/Home_Prices1_COS.csv?raw=1\n",
    "\n",
    "**Step 4:** Train the KMeans clustering model for 4 clusters and 5 iterations\n",
    "\n",
    "**Step 5: ** Load the RDD of dense vectors into a collection\n",
    "\n",
    "**Step 6: ** Predict the cluster for a select few data points i.e. elements 0, 18, 35, 6  and 15 of the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 (Step 2 done in question 1)\n",
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "house = sc.textFile(SparkFiles.get('file')).map(lambda line: line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_arrays = house.map(lambda line: array([float(x) for x in line]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "clusters = KMeans.train(house_arrays, 4, maxIterations=5, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5\n",
    "collection = house_arrays.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point: [ 12839.   2405.]\n",
      "Cluster: 0\n",
      "Point: [ 10000.   2200.]\n",
      "Cluster: 1\n",
      "Point: [ 8040.  1400.]\n",
      "Cluster: 1\n",
      "Point: [ 13104.   1800.]\n",
      "Cluster: 0\n",
      "Point: [ 10000.   2351.]\n",
      "Cluster: 1\n",
      "Point: [ 3049.   795.]\n",
      "Cluster: 2\n",
      "Point: [ 38768.   2725.]\n",
      "Cluster: 3\n",
      "Point: [ 16250.   2150.]\n",
      "Cluster: 0\n",
      "Point: [ 43026.   2724.]\n",
      "Cluster: 3\n",
      "Point: [ 44431.   2675.]\n",
      "Cluster: 3\n",
      "Point: [ 40000.   2930.]\n",
      "Cluster: 3\n",
      "Point: [ 1260.   870.]\n",
      "Cluster: 2\n",
      "Point: [ 15000.   2210.]\n",
      "Cluster: 0\n",
      "Point: [ 10032.   1145.]\n",
      "Cluster: 1\n",
      "Point: [ 12420.   2419.]\n",
      "Cluster: 0\n",
      "Point: [ 69696.   2750.]\n",
      "Cluster: 3\n",
      "Point: [ 12600.   2035.]\n",
      "Cluster: 0\n",
      "Point: [ 10240.   1150.]\n",
      "Cluster: 1\n",
      "Point: [ 876.  665.]\n",
      "Cluster: 2\n",
      "Point: [ 8125.  1430.]\n",
      "Cluster: 1\n",
      "Point: [ 11792.   1920.]\n",
      "Cluster: 0\n",
      "Point: [ 1512.  1230.]\n",
      "Cluster: 2\n",
      "Point: [ 1276.   975.]\n",
      "Cluster: 2\n",
      "Point: [ 67518.   2400.]\n",
      "Cluster: 3\n",
      "Point: [ 9810.  1725.]\n",
      "Cluster: 1\n",
      "Point: [ 6324.  2300.]\n",
      "Cluster: 1\n",
      "Point: [ 12510.   1700.]\n",
      "Cluster: 0\n",
      "Point: [ 15616.   1915.]\n",
      "Cluster: 0\n",
      "Point: [ 15476.   2278.]\n",
      "Cluster: 0\n",
      "Point: [ 13390.    2497.5]\n",
      "Cluster: 0\n",
      "Point: [ 1158.   725.]\n",
      "Cluster: 2\n",
      "Point: [ 2000.   870.]\n",
      "Cluster: 2\n",
      "Point: [ 2614.   730.]\n",
      "Cluster: 2\n",
      "Point: [ 13433.   2050.]\n",
      "Cluster: 0\n",
      "Point: [ 12500.   3330.]\n",
      "Cluster: 0\n",
      "Point: [ 15750.   1120.]\n",
      "Cluster: 0\n",
      "Point: [ 13996.   4100.]\n",
      "Cluster: 0\n",
      "Point: [ 10450.   1655.]\n",
      "Cluster: 1\n",
      "Point: [ 7500.  1550.]\n",
      "Cluster: 1\n",
      "Point: [ 12125.   2100.]\n",
      "Cluster: 0\n",
      "Point: [ 14500.   2100.]\n",
      "Cluster: 0\n",
      "Point: [ 10000.   1175.]\n",
      "Cluster: 1\n",
      "Point: [ 10019.    2047.5]\n",
      "Cluster: 1\n",
      "Point: [ 48787.   3998.]\n",
      "Cluster: 3\n",
      "Point: [ 53579.   2688.]\n",
      "Cluster: 3\n",
      "Point: [ 10788.   2251.]\n",
      "Cluster: 1\n",
      "Point: [ 11865.   1906.]\n",
      "Cluster: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 6\n",
    "for point in collection:\n",
    "    print('Point:', point)\n",
    "    print('Cluster:', clusters.predict(point))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Spark Coding Challenge #2.ipynb",
   "provenance": [
    {
     "file_id": "1XOL2jQ8O6w5j3HpuD_41l0GLgX4Nropk",
     "timestamp": 1529420575011
    },
    {
     "file_id": "1ohgu-I-gCiGgo0tZRkm8qJ5Gy8g2btTy",
     "timestamp": 1529420420062
    },
    {
     "file_id": "1ynYAlO3MZLiZoIJq-mU8mCKRhN1Znxxd",
     "timestamp": 1528984378749
    },
    {
     "file_id": "1WkgmnBd3wTIW5DR1jvokNlfnCQNrfR9l",
     "timestamp": 1528979233025
    },
    {
     "file_id": "1oJCI_YSdpHmHwcpGYmnB5c-LBXh50MHM",
     "timestamp": 1528924839108
    },
    {
     "file_id": "15VZIkdnCuuYPwgQYuf958E09sP4lTISb",
     "timestamp": 1528642554887
    },
    {
     "file_id": "1ze4oqASRYRH4sM47GcSEvsIy9HlVxi6p",
     "timestamp": 1528579590698
    },
    {
     "file_id": "1-EV69_DVCzbpMWLiPYHSroeL91624q7j",
     "timestamp": 1528555981083
    },
    {
     "file_id": "1O-exHskPOjgEs1Hrm7utpkHJ-bLmWxbS",
     "timestamp": 1528507363505
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
