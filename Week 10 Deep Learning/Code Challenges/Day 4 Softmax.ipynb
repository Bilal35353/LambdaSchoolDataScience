{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Deep Learning CC #2.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1ZhAUlZiS2-1XpkYjmzfvP3_A25HTjieA",
          "timestamp": 1528383519613
        },
        {
          "file_id": "1_j9SjDzGglSpGlBfZAH7Nhqkb5om0uaN",
          "timestamp": 1528382778982
        }
      ],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "MuqomGj-WkF-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Softmax\n",
        "\n",
        "The Softmax Function is the final step in a CNN, it takes the outputs from the final layer of neurons and transforms them to be probability values between 0 and 1. In this way, instead of a vector of voting weights, we end up with a vector of probabilities where there is a single probability for each label/class. This is how our CNN makes its final classification predictions.\n",
        "\n",
        "From Wikipedia: [Softmax Function](https://en.wikipedia.org/wiki/Softmax_function)\n",
        "\n",
        "\"The Softmax Function... is a generalization of the logistic function that \"squashes\" a K-dimensional vector of arbitrary real values to a K-dimensional vector of real values, where each entry is in the range (0, 1), and all the entries add up to 1.\"\n",
        "\n",
        "$S(y_i) = \\frac{e^{y_i}}{\\sum_{j=1}^Je^{y_j}}$\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9BOIVMc5WlyX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Do It\n",
        "\n",
        "Write your own softmax function from scratch that will work on both 1-D and 2-D matrices. \n",
        "\n",
        "The following inputs should yield the given outputs:\n",
        "\n",
        "### 1-D input: \n",
        "\n",
        "$\\begin{bmatrix}\n",
        "  1 & 2 & 3 & 6 \\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        "### 1-D output: \n",
        "\n",
        " Notice that the values in this matrix add up to 1 and are scaled exponentially.\n",
        "\n",
        "$\\begin{bmatrix}\n",
        "  0.00626879 & 0.01704033 & 0.04632042 & 0.93037047 \\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        "### 2-D input:\n",
        "\n",
        "$\\begin{bmatrix}\n",
        "  1 & 2 & 3 & 6 \\\\\n",
        "  2 & 4 & 5 & 6 \\\\\n",
        "  3 & 8 & 7 & 6 \\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        "### 2-D output:\n",
        "\n",
        "Notice that each row in the 2-D output adds up to 1. \n",
        "\n",
        "$\\begin{bmatrix}\n",
        "  0.00626879 & 0.01704033 &  0.04632042 & 0.93037045] \\\\\n",
        "  0.01203764 & 0.08894681 & 0.24178252 & 0.65723302 \\\\\n",
        "   0.00446236 & 0.66227241 & 0.24363641 & 0.08962882 \\\\\n",
        "\\end{bmatrix}$"
      ]
    },
    {
      "metadata": {
        "id": "A8GaKzmSwqwO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CV3Omo3bWe-K",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def softmax(X):\n",
        "    if type(X)!=np.ndarray:\n",
        "        X = np.array(X)\n",
        "    if len(X.shape)==1:\n",
        "        X = X.reshape(1, -1)\n",
        "    \n",
        "        \n",
        "    top = np.exp(X)\n",
        "    bottom = np.sum(top, axis=1).reshape(-1, 1)\n",
        "    \n",
        "    return top/bottom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NBdJFp-wxQfP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2c58e451-bfff-49b6-9142-e2562d93aa23",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528384279219,
          "user_tz": 420,
          "elapsed": 396,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "X = [1, 2, 3, 6]\n",
        "print('X:', X)\n",
        "print('softmax(X):', softmax(X))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('X:', [1, 2, 3, 6])\n",
            "('softmax(X):', array([[0.00626879, 0.01704033, 0.04632042, 0.93037047]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sgYIIUENxURL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "7dbafd7b-f54d-4143-de3e-50c3d5fc019e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528384279953,
          "user_tz": 420,
          "elapsed": 388,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "X = np.array([[1, 2, 3, 6], [2, 4, 5, 6], [3, 8, 7, 6]])\n",
        "print('X:', X)\n",
        "print('softmax(X):', softmax(X))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('X:', array([[1, 2, 3, 6],\n",
            "       [2, 4, 5, 6],\n",
            "       [3, 8, 7, 6]]))\n",
            "('softmax(X):', array([[0.00626879, 0.01704033, 0.04632042, 0.93037047],\n",
            "       [0.01203764, 0.08894682, 0.24178252, 0.65723302],\n",
            "       [0.00446236, 0.66227241, 0.24363641, 0.08962882]]))\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}