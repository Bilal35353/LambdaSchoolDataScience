{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Networks Sprint Challenge.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1hg14gC4i-NlVzW6yBlinKodKaL7zkbDt",
          "timestamp": 1527866222747
        },
        {
          "file_id": "1xuLPji93MPmkVTQIgq1YLCWUH0P2WKCj",
          "timestamp": 1527859629942
        },
        {
          "file_id": "1ZvTSgIadSujpndPYx_Zpysz0xDvOCXQg",
          "timestamp": 1526076252059
        },
        {
          "file_id": "1l2v-TFUpPYc746ZSv24U97i1g0TYhdgs",
          "timestamp": 1526009592603
        },
        {
          "file_id": "1Ig9oLMQaJn0zM48BTqrZxM8F5QUHGAME",
          "timestamp": 1526008523057
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "aD8Yb0P8D04e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Networks Sprint Challenge\n",
        "\n",
        "Objectives:\n",
        "  * Write a simple neural network with hidden layers\n",
        "  * Perform forward propagation to make predictions on data with the network\n",
        "  * Perform backward propagation of errors in the network\n",
        "  * Use MLPClassifier to train and predict on a dataset\n",
        "\n",
        "### Background\n",
        "\n",
        "Other than the MLPClassifier objective, you will be working with this neural net during this coding challenge:\n",
        "\n",
        "![Simple Neural Net](https://www.lucidchart.com/publicSegments/view/a5b0773e-7165-450d-99fc-7089891e099a/image.png)"
      ]
    },
    {
      "metadata": {
        "id": "9JrCk2HEPwoI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1. Write a simple three layer network\n",
        "\n",
        "Create variables to store weights and biases for the above network. Initialize each with $0.5$."
      ]
    },
    {
      "metadata": {
        "id": "5UeD3N5_PwCF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g7KPK64KK2g6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "params = {'w0': 0.5, 'b0': 0.5,\n",
        "          'w1': 0.5, 'b1': 0.5,\n",
        "          'w2': 0.5, 'b2': 0.5}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jSfpHHfJFPc5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Compute forward propagation for a new sample in three layer network\n",
        "\n",
        "Write a function `feed_forward` that takes a new sample $x$ and calculates $\\hat{y}$ via forward propagation."
      ]
    },
    {
      "metadata": {
        "id": "oI260RxPrUV7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "x1 = 4\n",
        "x2 = 0.5\n",
        "x3 = 0.125\n",
        "y1 = 0\n",
        "y2 = 1\n",
        "y3 = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0foYVmsCK4nQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "f7273f65-4482-4e1d-f6b1-0263b23d1efc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527870913390,
          "user_tz": 420,
          "elapsed": 551,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "plt.scatter([x1, x2, x3], [y1, y2, y3]);"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7ff8242b67b8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "x3K306_pFUQc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def feed_forward(x):\n",
        "    \n",
        "    h1 = sigmoid(params['w0']*x + params['b0'])\n",
        "    h2 = sigmoid(params['w1']*h1 + params['b1'])\n",
        "    yhat = sigmoid(params['w2']*h2 + params['b2'])\n",
        "    \n",
        "    return yhat\n",
        "\n",
        "# TEST\n",
        "y_hat1 = feed_forward(x1)\n",
        "y_hat2 = feed_forward(x2)\n",
        "y_hat3 = feed_forward(x3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ObNgyaLj_I3x",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a231bd54-6630-4989-c341-3957e025e740",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527870914828,
          "user_tz": 420,
          "elapsed": 377,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print('y_hat1: {}\\ny_hat2: {}\\ny_hat3: {}'.format(y_hat1, y_hat2, y_hat3))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_hat1: 0.7030299333006731\n",
            "y_hat2: 0.7003970647199883\n",
            "y_hat3: 0.6999291610175293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xS6VlmpYTzPx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. Compute backward propagation for the same network\n",
        "\n",
        "The backprop algorithm is derived from the goal of minimizing the error (or loss) function $\\epsilon = (y - \\hat{y})^2$.\n",
        "\n",
        "$\\epsilon = (y - \\sigma(h_2+b_2))^2$\n",
        "\n",
        "Via the chain rule, the derivative of the above is\n",
        "\n",
        "$\\frac{\\partial \\epsilon}{\\partial \\hat{y}} = 2(\\hat{y}-y)$\n",
        "\n",
        "Let $\\alpha = 0.1$. Update the weights for $h_2$ and $h_1$ via back propagation so that $w_{h2}$ = $w_{h2} + \\alpha \\frac{\\partial \\epsilon}{\\partial w_{h2}}$ and $w_{h1} = w_{h1} + \\alpha \\frac{\\partial \\epsilon}{\\partial w_{h1}}$\n",
        "\n",
        "Also, let $\\sigma(x) = ReLU(x)$. As such, $\\sigma'(x) = 0$ when $x \\le 0$ and $\\sigma'(x) = 1$ when $x \\gt 0$.\n",
        "\n",
        "Check Case1: of [Brian Dolhansky](http://briandolhansky.com/blog/2013/9/27/artificial-neural-networks-backpropagation-part-4) for a more detailed explanation of the values in the back propagation.\n"
      ]
    },
    {
      "metadata": {
        "id": "X9E99PyZBQGu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since the output layer is still performing binary classification, I will use a sigmoid activation there. Also, in favor of interpretibility, I will explicitly make variables for each partial derivative I will be chaining together, even if some of them are equal to 1, or to existing parameters.\n",
        "\n",
        "![](http://www.rayheberer.ai/img/lambdaschool/3layer.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "79jIb5IkAYlJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def feed_forward_and_back_propagate(x, y, alpha=0.1):\n",
        "    global params\n",
        "    \n",
        "    # forward pass\n",
        "    z1 = params['w0']*x + params['b0']\n",
        "    h1 = relu(z1)\n",
        "    \n",
        "    z2 = params['w1']*h1 + params['b1']\n",
        "    h2 = relu(z2)\n",
        "    \n",
        "    z3 = params['w2']*h2 + params['b2']\n",
        "    yhat = sigmoid(z3)\n",
        "    \n",
        "    # backpropagation, output layer\n",
        "    dE_dyhat = yhat - y\n",
        "    dyhat_dz3 = sigmoid(z3) * (1-sigmoid(z3))\n",
        "    dE_dz3 = dE_dyhat * dyhat_dz3\n",
        "    \n",
        "    dz3_dw2 = h2\n",
        "    dz3_db2 = 1\n",
        "    \n",
        "    # h2 layer\n",
        "    dz3_dh2 = params['w2']\n",
        "    dh2_dz2 = 1 if z2 > 0 else 0\n",
        "    dE_dz2 = dE_dz3 * dz3_dh2 * dh2_dz2\n",
        "    \n",
        "    dz2_dw1 = h1\n",
        "    dz2_db1 = 1\n",
        "    \n",
        "    # h1 layer\n",
        "    dz2_dh1 = params['w1']\n",
        "    dh1_dz1 = 1 if z1 > 0 else 0\n",
        "    dE_dz1 = dE_dz2 * dz2_dh1 * dh1_dz1\n",
        "    \n",
        "    dz1_dw0 = x\n",
        "    dz1_db0 = 1\n",
        "    \n",
        "    # derivatives with respect to weights and biases via chain rule\n",
        "    deltas = {'w0': dE_dz1 * dz1_dw0, \n",
        "              'b0': dE_dz1 * dz1_db0,\n",
        "              'w1': dE_dz2 * dz2_dw1,\n",
        "              'b1': dE_dz2 * dz2_db1,\n",
        "              'w2': dE_dz3 * dz3_dw2,\n",
        "              'b2': dE_dz3 * dz3_db2}\n",
        "    \n",
        "    for param in params.keys():\n",
        "        params[param] = params[param] - alpha*deltas[param]\n",
        "        \n",
        "    return yhat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fv_8vZvrT44v",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# CODE\n",
        "y_hat4 = feed_forward_and_back_propagate(x1,y1)\n",
        "y_hat5 = feed_forward_and_back_propagate(x2,y2)\n",
        "y_hat6 = feed_forward_and_back_propagate(x3,y3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tn17kqrRIBZG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b901d612-fc52-4be0-9f3a-83605454af3b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527870916804,
          "user_tz": 420,
          "elapsed": 424,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print('y_hat4: {}\\ny_hat5: {}\\ny_hat6: {}'.format(y_hat4, y_hat5, y_hat6))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_hat4: 0.7981867777396212\n",
            "y_hat5: 0.7096944415850966\n",
            "y_hat6: 0.7034645495997112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DFDL0iYuMl1l",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# copying the initialization code block so I can run multiple trials\n",
        "\n",
        "params = {'w0': 0.5, 'b0': 0.5,\n",
        "          'w1': 0.5, 'b1': 0.5,\n",
        "          'w2': 0.5, 'b2': 0.5}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VheSsUKlHeWJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "7651328f-c4a2-4524-f5fb-8b528ae6a28b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527870918591,
          "user_tz": 420,
          "elapsed": 383,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "X = [x1, x2, x3]\n",
        "Y = [y1, y2, y3]\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for x, y in zip(X, Y):\n",
        "        feed_forward_and_back_propagate(x, y)\n",
        "        \n",
        "    if (epoch+1) % 100 == 0 or epoch==0:\n",
        "        sse = 0\n",
        "        accuracy = 0\n",
        "        \n",
        "        for x, y in zip(X, Y):\n",
        "            yhat = feed_forward_and_back_propagate(x, y)\n",
        "            sse += (y-yhat)**2\n",
        "            accuracy += (np.round(yhat)==y)\n",
        "            \n",
        "        accuracy /= len(X)\n",
        "        print('epoch {}\\nSSE: {}\\nAccuracy: {:.1f}%\\n'.format(epoch+1, \n",
        "                                                             sse, \n",
        "                                                             accuracy*100))\n",
        "\n",
        "y_hat4 = feed_forward_and_back_propagate(x1,y1)\n",
        "y_hat5 = feed_forward_and_back_propagate(x2,y2)\n",
        "y_hat6 = feed_forward_and_back_propagate(x3,y3)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1\n",
            "SSE: 0.8002687651665472\n",
            "Accuracy: 66.7%\n",
            "\n",
            "epoch 100\n",
            "SSE: 0.6745517349612775\n",
            "Accuracy: 66.7%\n",
            "\n",
            "epoch 200\n",
            "SSE: 0.6714364081819317\n",
            "Accuracy: 66.7%\n",
            "\n",
            "epoch 300\n",
            "SSE: 0.6707608546397539\n",
            "Accuracy: 66.7%\n",
            "\n",
            "epoch 400\n",
            "SSE: 0.6698981687465115\n",
            "Accuracy: 66.7%\n",
            "\n",
            "epoch 500\n",
            "SSE: 0.6621386020524618\n",
            "Accuracy: 66.7%\n",
            "\n",
            "epoch 600\n",
            "SSE: 0.5654037644051342\n",
            "Accuracy: 66.7%\n",
            "\n",
            "epoch 700\n",
            "SSE: 0.2996171799453438\n",
            "Accuracy: 100.0%\n",
            "\n",
            "epoch 800\n",
            "SSE: 0.12848935343971607\n",
            "Accuracy: 100.0%\n",
            "\n",
            "epoch 900\n",
            "SSE: 0.067331239685979\n",
            "Accuracy: 100.0%\n",
            "\n",
            "epoch 1000\n",
            "SSE: 0.04212986857517585\n",
            "Accuracy: 100.0%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PYmuA8VwJ4_y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4. Use MLPClassifier to train a dataset\n",
        "\n",
        "`X` is now a small dataset. Create an MLPClassifier from sklearn and train it on the `X` dataset, with `y` as the targets."
      ]
    },
    {
      "metadata": {
        "id": "Jcyi6Z16OGuM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1686
        },
        "outputId": "2f9475cf-64e5-4ddb-9819-08193f85b3b8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527870920079,
          "user_tz": 420,
          "elapsed": 390,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "X = np.row_stack([x1,x2,x3])\n",
        "y = np.row_stack([y1,y2,y3])\n",
        "\n",
        "model = MLPClassifier(\n",
        "                    hidden_layer_sizes=(15, 2),\n",
        "                    activation='logistic',\n",
        "                    solver='sgd',\n",
        "                    alpha=1e-5,\n",
        "                    batch_size=3, \n",
        "                    learning_rate='adaptive',\n",
        "                    learning_rate_init=1,\n",
        "                    max_iter=200,\n",
        "                    shuffle=True,\n",
        "                    random_state=42,\n",
        "                    verbose=10,\n",
        "                    tol=1e-4 )\n",
        "\n",
        "model.fit(X, y)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.74575552\n",
            "Iteration 2, loss = 0.64876748\n",
            "Iteration 3, loss = 0.64309995\n",
            "Iteration 4, loss = 0.64810527\n",
            "Iteration 5, loss = 0.63680024\n",
            "Iteration 6, loss = 0.61984340\n",
            "Iteration 7, loss = 0.60648552\n",
            "Iteration 8, loss = 0.59519955\n",
            "Iteration 9, loss = 0.58077987\n",
            "Iteration 10, loss = 0.55956157\n",
            "Iteration 11, loss = 0.52995884\n",
            "Iteration 12, loss = 0.49142231\n",
            "Iteration 13, loss = 0.44314405\n",
            "Iteration 14, loss = 0.38546625\n",
            "Iteration 15, loss = 0.32253528\n",
            "Iteration 16, loss = 0.26039490\n",
            "Iteration 17, loss = 0.20430949\n",
            "Iteration 18, loss = 0.15736909\n",
            "Iteration 19, loss = 0.12024467\n",
            "Iteration 20, loss = 0.09196359\n",
            "Iteration 21, loss = 0.07085570\n",
            "Iteration 22, loss = 0.05520649\n",
            "Iteration 23, loss = 0.04357135\n",
            "Iteration 24, loss = 0.03485039\n",
            "Iteration 25, loss = 0.02824640\n",
            "Iteration 26, loss = 0.02319177\n",
            "Iteration 27, loss = 0.01928255\n",
            "Iteration 28, loss = 0.01622904\n",
            "Iteration 29, loss = 0.01382125\n",
            "Iteration 30, loss = 0.01190530\n",
            "Iteration 31, loss = 0.01036731\n",
            "Iteration 32, loss = 0.00912217\n",
            "Iteration 33, loss = 0.00810577\n",
            "Iteration 34, loss = 0.00726941\n",
            "Iteration 35, loss = 0.00657587\n",
            "Iteration 36, loss = 0.00599642\n",
            "Iteration 37, loss = 0.00550882\n",
            "Iteration 38, loss = 0.00509566\n",
            "Iteration 39, loss = 0.00474325\n",
            "Iteration 40, loss = 0.00444077\n",
            "Iteration 41, loss = 0.00417958\n",
            "Iteration 42, loss = 0.00395275\n",
            "Iteration 43, loss = 0.00375469\n",
            "Iteration 44, loss = 0.00358085\n",
            "Iteration 45, loss = 0.00342751\n",
            "Iteration 46, loss = 0.00329163\n",
            "Iteration 47, loss = 0.00317067\n",
            "Iteration 48, loss = 0.00306254\n",
            "Iteration 49, loss = 0.00296547\n",
            "Iteration 50, loss = 0.00287800\n",
            "Iteration 51, loss = 0.00279889\n",
            "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.200000\n",
            "Iteration 52, loss = 0.00272706\n",
            "Iteration 53, loss = 0.00266572\n",
            "Iteration 54, loss = 0.00261138\n",
            "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.040000\n",
            "Iteration 55, loss = 0.00256312\n",
            "Iteration 56, loss = 0.00252084\n",
            "Iteration 57, loss = 0.00248340\n",
            "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.008000\n",
            "Iteration 58, loss = 0.00245019\n",
            "Iteration 59, loss = 0.00242080\n",
            "Iteration 60, loss = 0.00239469\n",
            "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.001600\n",
            "Iteration 61, loss = 0.00237147\n",
            "Iteration 62, loss = 0.00235082\n",
            "Iteration 63, loss = 0.00233241\n",
            "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000320\n",
            "Iteration 64, loss = 0.00231599\n",
            "Iteration 65, loss = 0.00230134\n",
            "Iteration 66, loss = 0.00228825\n",
            "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000064\n",
            "Iteration 67, loss = 0.00227654\n",
            "Iteration 68, loss = 0.00226607\n",
            "Iteration 69, loss = 0.00225670\n",
            "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000013\n",
            "Iteration 70, loss = 0.00224830\n",
            "Iteration 71, loss = 0.00224078\n",
            "Iteration 72, loss = 0.00223403\n",
            "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000003\n",
            "Iteration 73, loss = 0.00222798\n",
            "Iteration 74, loss = 0.00222256\n",
            "Iteration 75, loss = 0.00221769\n",
            "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000001\n",
            "Iteration 76, loss = 0.00221332\n",
            "Iteration 77, loss = 0.00220939\n",
            "Iteration 78, loss = 0.00220587\n",
            "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:912: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='logistic', alpha=1e-05, batch_size=3, beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=(15, 2), learning_rate='adaptive',\n",
              "       learning_rate_init=1, max_iter=200, momentum=0.9,\n",
              "       nesterovs_momentum=True, power_t=0.5, random_state=42, shuffle=True,\n",
              "       solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=10,\n",
              "       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "y7s_0m9HMAAE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db2a0902-a9f9-4ff7-e876-401927a83b74",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527870921526,
          "user_tz": 420,
          "elapsed": 346,
          "user": {
            "displayName": "Ray Heberer",
            "photoUrl": "//lh4.googleusercontent.com/-BMlr5I5Dhow/AAAAAAAAAAI/AAAAAAAAABc/XW4PF5A8K2Q/s50-c-k-no/photo.jpg",
            "userId": "116545933704048584401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "yhat = model.predict(X)\n",
        "accuracy = np.mean(yhat == y.flatten())\n",
        "\n",
        "print('MLP Accuracy: {:.0f}%'.format(accuracy*100))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP Accuracy: 100%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}